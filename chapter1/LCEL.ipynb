{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdQdAIlL_F5n",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture --no-stderr\n",
    "!pip install python-dotenv openAI langchain_core langchain_openai"
   ],
   "metadata": {
    "id": "jaK1P34-_QM5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 환경변수 설정"
   ],
   "metadata": {
    "id": "6XVQKIdS_SEM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 라이브러리 불러오기\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import OpenAI"
   ],
   "metadata": {
    "id": "h_OvhUI0_VIs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# .env 파일에서 환경 변수 로드\n",
    "load_dotenv(\"/content/.env\")\n",
    "# 환경 변수에서 API 키 가져오기\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# 오픈AI 대규모 언어 모델 초기화\n",
    "llm = OpenAI()"
   ],
   "metadata": {
    "id": "klFZL6kD_WZP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **호출(invoke) 출력**"
   ],
   "metadata": {
    "id": "fRgljW-y_fK1",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 라이브러리 설치\n",
    "!pip install langchain_core langchain_openai\n",
    "# 라이브러리 불러오기\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 오픈AI의 대규모 언어 모델 설정\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ],
   "metadata": {
    "id": "edtkwNEF_l2Y",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 프롬프트 템플릿 정의: 주어진 주제에 대한 설명 요청\n",
    "prompt = ChatPromptTemplate.from_template(\"주제 {topic}에 대해 짧은 설명을 해주세요.\")\n",
    "# 출력 파서 정의: AI 메시지의 출력 내용을 추출\n",
    "parser = StrOutputParser()\n",
    "# 프롬프트, 모델, 출력 파서를 체인으로 연결\n",
    "chain = prompt | model | parser"
   ],
   "metadata": {
    "id": "VdVuaWXCAGpN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 응답 호출\n",
    "chain.invoke({\"topic\": \"더블딥\"})"
   ],
   "metadata": {
    "id": "CH9xSFgvBCNj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **배치(Batch) 출력**"
   ],
   "metadata": {
    "id": "KVpIDIjLBtNp",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 주어진 주제 리스트를 배치로 출력\n",
    "chain.batch([{\"topic\": \"더블딥\"}, {\"topic\": \"인플레이션\"}])"
   ],
   "metadata": {
    "id": "_XNW7ylqBwmp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **스트림(Stream) 출력**"
   ],
   "metadata": {
    "id": "aWVbHGZDCRQb",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 응답을 토큰 단위로 스트리밍하여 출력\n",
    "for token in chain.stream({\"topic\": \"더블딥\"}):\n",
    "  # 스트리밍된 내용을 출력, 각 내용을 붙여서 출력하며 버퍼를 즉시 플러시하여 실시간으로 보여줌\n",
    "  print(token, end=\"\", flush=True)"
   ],
   "metadata": {
    "id": "JzB-DrOKCT06",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **구성된 체인을 다른 러너블과 결합하기**"
   ],
   "metadata": {
    "id": "AJZk-XIFBRUf",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 라이브러리 불러오기\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parser import StrOutputParser\n",
    "\n",
    "# 이 대답을 영어로 번역해 주세요 라는 질문을 생성하는 프롬프트 템플릿 정의\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"이 대답을 영어로 번역해 주세요: {answer}\")"
   ],
   "metadata": {
    "id": "LpEyWMPOBY2b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 이전에 정의된 체인과 새로운 작업을 연결하는 체인 구성\n",
    "composed_chain = {\"answer\": chain} | analysis_prompt | model | StrOutputParser()\n",
    "\n",
    "# 더블딥이라는 주제로 응답을 생성하고 체인 실행\n",
    "composed_chain.invoke({\"topic\": \"더블딥\"})"
   ],
   "metadata": {
    "id": "qfNzNKFbCCyz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **람다 함수를 사용한 체인을 통해 구성하기**"
   ],
   "metadata": {
    "id": "d8sD6BnDDGO3",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 이전에 정의된 값들\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "prompt = ChatPromptTemplate.from_template(\"{topic}에 대해 짧은 설명을 해주세요.\")\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"이 대답을 영어로 번역해 주세요: {answer}\")\n",
    "\n",
    "# 람다 함수를 사용한 체인 구성\n",
    "composed_chain_with_lambda = (\n",
    "    # 이전에 정의된 체인을 사용하여 입력된 데이터를 받아온다\n",
    "    chain\n",
    "    # 입력된 데이터를 answer 키로 변환하는 람다 함수를 적용한다\n",
    "    # 람다 함수 = 익명 함수\n",
    "    # 매개변수 : 함수 내용\n",
    "    # 즉 딕셔너리 반환\n",
    "    | (lambda input: {\"answer\": input})\n",
    "    # answer 키를 가진 데이터를 영어로 번역하도록 프롬프트에 전달한다\n",
    "    | analysis_prompt\n",
    "    # 프롬프트에서 생성된 요청을 모델에 전달하여 결과를 생성한다\n",
    "    | model\n",
    "    # 모델에서 변환된 결과를 문자열로 파싱한다\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "#  더빌딥이라는 주제로 답변을 생성하고 답변을 영어로 번역한다\n",
    "composed_chain_witg_lambda.invoke({\"topic\": \"더블딥\"})"
   ],
   "metadata": {
    "id": "yfKX8yPZDJNo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **.pipe()를 통해 체인 구성하기**"
   ],
   "metadata": {
    "id": "jnD0EWP-GyAN",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 방법1 여러 작업을 순차적으로 pipe를 통해 연결하여 체인 구성하기\n",
    "composed_chain_with_pipe = (\n",
    "    # 이전에 정의된 체인으로 입력된 데이터를 받아옴\n",
    "    chain\n",
    "    # 입력된 데이터를 answer 키로 변환하는 람다 함수 적용\n",
    "    .pipe(lambda input: {\"answer\": input})\n",
    "    # analysis_prompt를 체인에 연결하여 설명을 영어로 번역하는 작업 추가\n",
    "    .pipe(analysis_prompt)\n",
    "    # 모델을 사용해 응답 생성\n",
    "    .pipe(model)\n",
    "    # 생성된 응답을 문자열로 파싱\n",
    "    .pipe(StrOutputParser())\n",
    ")\n",
    "\n",
    "# 더블딥이라는 주제로 체인을 실행하여 답변 생성\n",
    "composed_chain_with_pipe.invoke({\"topic\": \"더블딥\"})"
   ],
   "metadata": {
    "id": "CMdd4mPjGzxG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 방법2 좀 더 간단하게 연결하기\n",
    "composed_chain_with_pipe = chain.pipe(lambda input: {\"answer\": input}, analysis_prompt, model, StrOutputParser())\n",
    "\n",
    "# 더블딥이라는 주제로 체인을 실행하여 답변 생성\n",
    "composed_chain_with_pipe.invoke({\"topic\": \"더블딥\"})"
   ],
   "metadata": {
    "id": "YZItDOX4HjXh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}