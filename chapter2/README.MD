# **검색 증강 생성 - 기초와 실습**  
랭체인은 다양한 인공지능 애플리케이션 개발에 사용되지만 그중에서도 가장 널리 활용되는 사용처 중 하나는 검색 증강 생성(RAG)다. 검색 증강 생성은 
사용자가 질문을 입력하면 연관된 문서를 검색한 후 검색 결과를 바탕으로 응답을 생성하는 방식이다.  
  
대형 데이터셋으로 학습된 언어 모델들은 고정된 정보만을 다루기 때문에 최신 정보나 특정 개인 데이터를 반영하는 데 한계가 있다. 검색 증강 생성은 데이터 
내 검식기(retriever)를 활용하여 최신 정보나 특정 문서를 모델에 제공함으로써 더욱 정확하고 유용한 답변을 생성하도록 지원한다.  
  
![img.png](image/img.png)  
  
위 그림에서 볼 수 있듯이 검색 증강 생성은 크게 인덱싱 과정과 쿼리 과정으로 나뉜다. 인덱싱 과정은 문서를 불러온 후 텍스트를 적절히 분할하고 임베딩하여 
벡터 DB에 저장하는 단계다. 그리고 쿼리 과정에서 사용자의 질문이 입력되면 검색기를 통해 관련 정보를 찾아 이를 바탕으로 응답을 생성한다. 다른 문헌에서는 
쿼리 과정을 검색과 생성으로 표현하기도 한다. 이 두 가지 단계를 통해 보다 풍부하고 맥락에 맞는 응답을 제공할 수 있다.  
  
일반적으로 대규모 언어 모델을 특정 도메인에 맞춰 미세 조정하는(fine-tuning) 방법도 있지만 이 과정은 비용이 많이 들고 정보의 신뢰성을 보장하기 어려운 
한계가 있다. 반면 검색 증강 생성은 검색된 정보를 실시간으로 모델에 제공하여 모델이 이를 바탕으로 최종 응답을 생성하게 한다. 덕분에 정보의 정확성과 
관련성이 높아져 더욱 신뢰할 수 있는 결과를 얻을 수 있다.  
  
# **검색 증강 생성 개요**  
랭체인은 인공지능 애플리케이션 개발에 다양하게 활용되며 그 중에서도 검색 증강 생성 기술이 특히 주목받고 있다.  
  
# **텍스트 임베딩**  
인공지능 모델은 수많은 숫자로 이루어진 벡터를 사용하여 데이터를 처리하고 학습한다. 예를 들어 백터 [1. 0.3, 0.5, 2]와 같이 4개의 숫자를 지닌 벡터는 정보를 
수치로 표현한 형태이다. 사람들이 텍스트를 읽고 내용을 이해하는 방식과는 다르게 인공지능 모델은 텍스트를 직접 처리하지 않는다. 대신 텍스트를 수치화된 
벡터로 변환하여 이를 분석한다. 이러한 변환 과정을 임베딩이라고 부른다.  
  
임베딩에는 여러 형태가 있다. 단어를 벡터로 변환하는 워드 임베딩(word embedding)과 문장이나 문서를 벡터로 변환하는 문장 임베딩(sentence embedding) 
또는 문서 임베딩(document embedding)이 대표적이다. 예를 들어 특정 단어나 문장, 문서는 [1, 0.3, 0.5]와 같은 실수 벡터로 인코딩된다. 모델은 이렇게 
변환된 벡터를 통해 텍스트의 의미를 분석하고 활용한다.  
  
텍스트를 벡터로 변환하면 챗봇과 같은 시스템은 사용자의 질문에 대해 관련성 높은 답변을 빠르고 정확하게 검색하고 제공할 수 있다. 이러한 기술은 AI 
시스템의 효율성과 정확성을 크게 향상시킨다.  
  
텍스트를 벡터로 변환하는 과정은 다양한 인공지능 모델을 활용하여 수행할 수 있다. 랭체인에서 제공하는 임베딩 API로 텍스트를 숫자 벡터로 변환하면 
데이터를 수치화하여 처리할 수 있으며 벡터간 유사도 계산을 통해 텍스트의 의미적 연관성을 파악할 수 있다.  
  
# **코사인 유사도**  
코사인 유사도(cosine similarity)는 두 벡터 간의 각도를 통해 유사성을 측정하는 방법이다. 코사인 유사도는 벡터의 크기가 아니라 방향성에 초점을 맞추며 
-1에서 1사이의 값을 가진다.  
  
- 1: 두 벡터가 동일한 방향을 향함  
- 0: 두 벡터가 서로 직각을 이룸  
- -1: 두 벡터가 정반대 방향을 향함  
  
두 벡터 A와 B의 코사인 유사도는 다음 공식으로 계산한다.  
  
![img.png](image/img2.png)  
  
여기서 A B 는 두 벡터의 내적을 나타내고 || A || 와 || B ||는 각각 벡터 A와 B의 크기(놈, norm)이다. 코사인 유사도에서 두 벡터의 방향성을 기반으로 
유사성을 평가하므로 데이터 크기와 무관하게 텍스트나 문서의 의미적 유사성을 측정할 수 있다. 따라서 텍스트 분석, 문서 비교, 추천 시스템 등 다양한 
분야에서 활용된다.  
  
코사인 유사도를 계산할 때 복잡한 수학 공식으로 직접 계산할 필요는 없다. 이미 개발된 코드나 라이브러리를 사용하면 간편하게 구할 수 있다.  
  
VECTOR_SIMILARITY.ipynb(벡터들 간 코사인 유사도 계산)  
  
Numpy 라이브러리를 사용하여 벡터 간의 코사인 유사도를 계산하는 cos_sim 함수를 정의하고 이를 사용하여 세 벡터 간의 유사도를 평가한다. 이 함수는 
벡터 간의 각도를 이용하여 유사도를 수치화한다.  
  
이런 유사도 계산은 텍스트 임베딩과 벡터화를 통해 얻은 데이터를 기반으로 챗봇 개발에서 사용자 질문에 가장 관련성 높은 문서를 식별하는 검색 메커니즘에 
핵심적으로 활용된다. 즉, 벡터 간 유사도를 통해 사용자의 질문과 가장 일치하는 정보를 바르고 정확하게 찾는 데 결정적인 역할을 한다.  
  
# **랭체인 임베딩 API 활용**  
랭체인에서 사용 가능한 임베딩 API는 크게 두 가지 유형으로 나눌 수 있다.  
  
첫 번째 유형은 LLM 제공사의 임베딩 모델로 오픈 AI의 임베딩 제품군에는 text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large 등이 
포함되어 다양한 언어 처리 작업에 효과적으로 사용할 수 있다. 오픈 AI 외에도 코히어, 미스트랄 AI등 여러 LLM 제공사에서 임베딩 모델을 제공하고 있다.  
  
두 번째 유형은 허깅페이스에서 제공하는 임베딩 모델이다. BAAI/bge-m3 모델은 BGE(BAAI General Embedding)임베딩 모델 중 하나로 중국어를 주로 다루지만 
한국어에도 좋은 성능을 보인다. 또한 nlpai-lab/KoE5 모델은 한국어 텍스트 검색에 최적화되어 있으며 다국어 모델인 Multilingual-e5-large를 기반으로 
조정된 모델이다.  
  
개발자들은 이러한 임베딩 모델 중에서 자신의 프로젝트에 적합한 솔루션을 선택하여 챗봇 개발에 효과적으로 활용할 수 있다.  
  
# **LLM 제공사의 임베딩 모델**  
LLM 제공사의 임베딩 모델은 오픈 AI를 포함한 다양한 업체에서 제공한다. 오픈 AI의 임베딩 모델 외에 다른 LLM 제공사의 임베딩 모델에 관심이 있다면 
다음 링크를 참고하여 필요한 모델을 적용할 수 있다.  
  
https://python.langchain.com/docs/integrations/text_embedding/  
  
LANGCHAIN_EMBEDDING.ipynb(오픈AI 임베딩을 활용한 벡터들 간 코사인 유사도 계산)  
  
먼저 텍스트를 벡터로 변환하는 데 필요한 라이브러리를 불러온다. 그리고 dotenv 라이브러리를 사용하여 환경 설정을 불러와 오픈AI API 키를 로드한다.  
  
OpenAIEmbeddings 클래스는 오픈AI의 임베딩 API를 활용하여 주어진 텍스트를 벡터로 변환하는 기능을 제공한다. 이번 실습에는 text-embedding-ada-002 
모델을 사용하여 "저는 배가 고파요"라는 문장을 벡터로 변환한다. OpenAIEmbeddings()로 임베딩 모델 객체인 embeddings를 선언하고 이후 embed_query()를 
사용하여 입력된 문장을 벡터로 변환할 수 있다. 이렇게 텍스트를 수치화된 벡터로 변환하면 이를 다양한 자연어 처리 작업에 활용할 수 있다.  
  
실행 결과를 보면 저는 배가 고파요 라는 문장이 text-embedding-ada-002 모델을 사용하여 1536개의 실수로 구성된 벡터로 성공적으로 변환되었음을 알 
수 있다. 이렇게 문장의 의미적 특성을 수치로 변환한 벡터들을 사용하면 코사인 유사도 계산을 통해 다른 텍스트와 유사성을 평가할 수 있다. 이러한 과정은 검색 
시스템이나 추천 시스템 개발에 필수다.  
  
이번 실습에서는 유사도 평가를 위해 임의의 문장 데이터를 포함하는 데이터프레임을 생성한다. 판다스(pandas) 라이브러리를 활용하여 여섯 개의 문장을 하나의 
열에 할당하여 6행 1열 구조의 데이터프레임 df를 생성한다. 데이터프레임은 엑셀과 유사한 구조여서 데이터를 쉽게 조작하고 분석할 수 있다.  
  
생성한 데이터프레임을 활용해 각 텍스트 데이터를 벡터로 변환하여 문장 간 유사도를 계산하고 결과를 분석하여 인사이트를 도출할 수 있다.  
  
LANGCHAIN_EMBEDDING.ipynb(오픈AI 임베딩을 활용한 문장들 간 코사인 유사도 계산)  
  
![img.png](image/img3.png)  
  
먼저 파이썬의 판다스 라이브러리를 사용하여 여러 종류의 시장 및 금융 관련 텍스트 데이터를 포함하는 데이터프레임을 생성한다. 이 데이터프레임은 
text라는 이름의 열 하나를 포함하며 각 행에는 시장 동향, 부동산 시장의 복잡성, 음악의 비트, 그리고 암호화폐 변동 등 다양한 주제에 관한 문장이 
담겨 있다. 이 데이터를 이용하여 다음 단계에서 텍스트를 벡터 형태로 변환하는 임베딩 과정을 수행한다.  
  
![img.png](image/img4.png)  
  
데이터프레임 df의 text열에 있는 각 텍스트 데이터를 get_embedding() 함수를 이용해 벡터로 변환한다. 변환된 벡터들은 데이터프레임의 새로운 
embedding 열에 저장된다.  
  
return_answer_cnadidate() 함수는 사용자의 검색어를 입력받아 이를 get_embedding() 함수를 통해 벡터로 변환하고 query_embedding 변수에 저장한다. 
이 벡터와 데이터프레임 df의 embedding 열에 저장된 벡터들 간의 코사인 유사도를 계산하여 의미적 유사성이 가장 높은 상위 세 개의 텍스트를 추출한다. 
이 과정은 벡터 간의 각도를 활용해 유사도를 측정하는 것으로 값이 높을수록 두 텍스트의 의미적 유사성이 크다는 것을 나타낸다.  
  
예를 들어 과일 값이 비싸다 라는 검색어에 대해 이 함수를 실행하면 관련성이 높은 문장, 예컨대 시장이나 오르다와 관련된 문장이 포함된 데이터가 반환될 수 
있다. 이는 임베딩과 유사도 계산만으로도 의미적 연결성이 높은 결과를 효과적으로 찾아낼 수 있음을 보여준다.  
  
예제에서는 판다스 라이브러리를 사용하여 텍스트 데이터를 테이블 형태로 관리했지만 실제 환경에서는 판다스 외에도 벡터 데이터를 효율적으로 관리할 수 있는 
벡터 데이터베이스, 예를 들어 Faiss나 Chroma 같은 도구를 사용한다.  
  
# **허깅페이스에서 제공하는 임베딩 모델**  
허깅페이스에서는 다양한 임베딩 모델을 제공한다. 이 중 bge-m3 모델을 사용하여 앞서 오픈 AI 임베딩을 활용한 예제와 유사한 방식으로 문장 간 유사도를 
계산하는 코드를 작성한다. bge-m3 모델은 특히 다양한 언어를 지원하며 한국어 처리에도 우수한 성능을 보인다.  
  
앞의 코드에서는 각 텍스트 데이터를 bge-m3 모델을 이용하여 벡터로 변환하고 이를 데이터프레임에 저장한다. 이를 통해 각 텍스트의 의미적 내용을 벡터화하여 
분석할 수 있다.  
  
