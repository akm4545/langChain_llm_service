{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " PDF 분할 작업을 위해 `unstructured`를 사용한다`unstructured` 를 위해 다음과 도구의 설치가 필요하다\n",
    "\n",
    "  - tesseract : 광학 문자 인식(OCR)을 위해 사용\n",
    "  - poppler : PDF 렌더링 및 처리\n",
    "\n",
    "  [poppler 설치 방법](https://pdf2image.readthedocs.io/en/latest/installation.html)과 [tesseract 설치 방법](https://tesseract-ocr.github.io/tessdoc/Installation.html)을 참고"
   ],
   "metadata": {
    "id": "gJcEMfxPyh1B",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **라이브러리 설치**"
   ],
   "metadata": {
    "id": "UtvxH7AczSbr",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture --no-stderr\n",
    "!sudo apt install tesseract-ocr\n",
    "!sudo apt install libtesseract-dev\n",
    "!sudo apt-get install poppler-utils"
   ],
   "metadata": {
    "id": "-6RqMVDDzUNy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture --no-stderr\n",
    "! pip install -U langchain openai chromadb langchain-experimental langchain_openai nltk pydantic lxml matplotlib chromadb tiktoken\n",
    "! pip install pillow == 11.1.0\n",
    "! pip install \"unstructured[all-docs]\" == 0.17.2"
   ],
   "metadata": {
    "id": "UEZ4LlIuzU8H",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "t8QPZ69Gz1IV",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "GAo7fozczbe2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일에서 환경 변수 로드\n",
    "load_dotenv(\"/content/.env\")"
   ],
   "metadata": {
    "id": "UTYLu09wzc4V",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **PDF 데이터 전처리**"
   ],
   "metadata": {
    "id": "iepOPr-Jg81P",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 파일 경로\n",
    "fpath = 'pdf 파일이 저장된 디렉토리 경로'\n",
    "fname = \"sample.pdf\""
   ],
   "metadata": {
    "id": "FfmAt_Qug-fo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "\n",
    "# nltk 필요 데이터 다운로드\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ],
   "metadata": {
    "id": "PZ6ncu4GhYNS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "import os\n",
    "\n",
    "# PDF에서 요소 추출\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=os.path.join(fpath, fname),\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    extract_image_block_output_dir=fpath,\n",
    ")"
   ],
   "metadata": {
    "id": "XhkzuROOhrLf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 텍스트, 테이블 추출\n",
    "tables = []\n",
    "texts = []\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "  if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "    # 테이블 요소 추가\n",
    "    tables.append(str(element))\n",
    "  elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "    # 텍스트 요소 추가\n",
    "    texts.append(str(element))"
   ],
   "metadata": {
    "id": "d7Pw8TqPi7KS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tables[0]\n",
    "texts[0]"
   ],
   "metadata": {
    "id": "B1kNrvERjpIT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **텍스트 및 테이블 요약**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 프롬프트 설정\n",
    "prompt_text = \"\"\"당신은 표와 텍스트를 요약하여 검색할 수 있도록 돕는 역할을 맡은 어시스턴트입니다.\n",
    "이 요약은 임베딩되어 원본 텍스트나 표 요소를 검색하는 데 사용될 것입니다.\n",
    "표 또는 텍스트에 대한 간결한 요약을 제공하여 검색에 최적화된 형태로 만들어 주세요. 표 또는 텍스트: {element} \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# 텍스트 요약 체인\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "# 제공된 텍스트에 대해 요약을 할 경우\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "\n",
    "# 요약을 원치 않을 경우\n",
    "# text_summaries = texts\n",
    "\n",
    "# 제공된 테이블에 적용\n",
    "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "table_summaries[0]\n",
    "text_summaries[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **base64 인코딩 전달**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# -> str = 타입 힌트, 반환값\n",
    "def encode_image(image_path) -> str:\n",
    "  # 이미지 base64 인코딩\n",
    "  # rb = 바이너리 읽기 모드\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    # .decode('utf-8') = 바이트를 문자열로 변환\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# 이미지의 base64 인코딩을 저장하는 리스트\n",
    "img_base64_list = []\n",
    "\n",
    "# 이미지를 읽어 bese64 인코딩 후 저장\n",
    "# os.listdir(fpath) = 해당 경로에 있는 파일 목록 불러오기\n",
    "for img_file in sorted(os.listdir(fpath)):\n",
    "  if img_file.endswith('.jpg')\n",
    "    img_path = os.path.join(fpath, img_file)\n",
    "    base64_image = encode_image(img_path)\n",
    "    img_base64_list.append(base64_image)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **이미지 요약문 생성**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def image_summarize(img_base64: str) -> str:\n",
    "  # 이미지 요약\n",
    "  chat = ChatOpenAI(model=\"gpt-4o\", max_tokens=1024)\n",
    "  prompt = \"\"\"\n",
    "  당신은 이미지를 요약하여 검색을 위해 사용할 수 있도록 돕는 어시스턴트입니다.\n",
    "  이 요약은 임베딩되어 원본 이미지를 검색하는 데 사용됩니다.\n",
    "  이미지 검색에 최적화된 간결한 요약을 작성하세요.\n",
    "  \"\"\"\n",
    "  msg = chat.invoke(\n",
    "      [\n",
    "          HumanMessage(\n",
    "              content=[\n",
    "                  {\"type\": \"text\", \"text\": prompt},\n",
    "                  {\n",
    "                      \"type\": \"image_url\",\n",
    "                      \"image_url\": {\n",
    "                          \"url\": f\"data:image/jpeg;base64,{img_base64}\"\n",
    "                      }\n",
    "                  }\n",
    "              ]\n",
    "          )\n",
    "      ]\n",
    "  )\n",
    "\n",
    "  return msg.content\n",
    "\n",
    "# 이미지 요약을 저장하는 리스트\n",
    "image_summaries = []\n",
    "\n",
    "for img_base64 in img_base64_list:\n",
    "  image_summary = image_summarize(img_base64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_summaries[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **벡터 저장소**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.retrievers import MultiVectorRetriever\n",
    "from langchain_core.stores import InMemoryStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# 분할한 텍스트들을 색인할 벡터 저장소\n",
    "vectorstore = Chroma(collection_name=\"multi_modal_rag\", embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# 원본 문서 저장을 위한 저장소 선언\n",
    "# docstore는 실제 텍스트나 원본 자료(요약 전 문장, 이미지 정보 등) 를 저장하는 역할\n",
    "docstore = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# 검색기\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=docstore,\n",
    "    id_key=id_key,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **벡터 저장소 저장**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# 원본 텍스트 데이터 저장\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "# 원본 테이블 데이터 저장\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "\n",
    "# 원본 이미지(base64) 데이터 저장\n",
    "img_ids = [str(uuid.uuid4()) for _ in img_base64_list]\n",
    "retriever.docstore.mset(list(zip(img_ids, img_base64_list)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.schema.document import Document\n",
    "\n",
    "# 텍스트 요약 벡터 저장\n",
    "summary_texts = [\n",
    "    # LangChain의 Document는 텍스트 + 메타데이터를 함께 담는 기본 단위\n",
    "    # 각 루프마다 Document 객체가 만들어지고 i와 s는 text_summaries를 순회하면서 추출\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    # i는 순회 인덱스, s는 컨텐츠\n",
    "    for i, s in enumerate(text_summaries)\n",
    "]\n",
    "\n",
    "# 즉 인덱스 리스트와 원본, 요약 벡터 데이터의 순서가 같아서 id값과 벡터 데이터를 맞추며 저장하느 ㄴ과정\n",
    "\n",
    "retriever.vecterstore.add_documents(summary_texts)\n",
    "\n",
    "# 테이블 요약 벡터 저장\n",
    "summary_tables = [\n",
    "    Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "    for i, s in enumerate(table_summaries)\n",
    "]\n",
    "\n",
    "retriever.vectorstore.add_documents(summary_table)\n",
    "\n",
    "# 이미지 요약 벡터 저장\n",
    "summary_img = [\n",
    "    Document(page_content=s, metadata={id_key: img_ids[i]})\n",
    "    for i, s in enumerate(image_summaries)\n",
    "]\n",
    "\n",
    "retriever.vectorstore.add_documents(summary_img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **검색 결과 확인**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\n",
    "    \"말라리아 군집 사례는 어떤가요?\"\n",
    ")\n",
    "\n",
    "len(docs)\n",
    "\n",
    "# 4\n",
    "# 4개의 검색 결과"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from base64 import b64decode\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "  # 이미지와 텍스트 데이터를 분리\n",
    "  b64 = []\n",
    "  text = []\n",
    "\n",
    "  for doc in docs:\n",
    "    try:\n",
    "      b64decode(doc)\n",
    "      b64.append(doc)\n",
    "    except Exception as e:\n",
    "      text.append(doc)\n",
    "\n",
    "  return {\n",
    "      \"images\": b64,\n",
    "      \"texts\": text\n",
    "  }\n",
    "\n",
    "docs_by_type = split_image_text_types(docs)\n",
    "\n",
    "len(docs_by_type[\"images\"])\n",
    "# 3\n",
    "\n",
    "len(docs_by_type[\"texts\"])\n",
    "# 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "frp, IPython.display import display, HTML\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "  # base64 이미지로 html 태그를 작성\n",
    "  image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "\n",
    "  # html 태그를 기반으로 이미지를 표기\n",
    "  display(HTML(image_html))\n",
    "\n",
    "plt_img_base64(docs_by_type[\"images\"][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "docs_by_type[\"texts\"][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **답변 생성**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "# RunnablePassthrough(): 입력을 그대로 다음 단계로 넘겨주는 역할(일종의 식별자/포맷 보존)\n",
    "# RunnableLambda(fn): 사용자가 제공한 함수 fn을 래핑해서 runnable로 만든다. 실행 시 해당 함수를 호출합니다.\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "def prompt_func(dict):\n",
    "  # 문맥 텍스트 리스트를 줄바꿈으로 연결해 하나의 긴 텍스트로 만든다\n",
    "  # 검색 결과들 합침\n",
    "  format_texts = \"\\n\".join(dict[\"context\"][\"texts\"])\n",
    "  text = f\"\"\"\n",
    "  다음 문맥에만 기반하여 질문에 답하세요. 문맥에는 텍스트, 표, 그리고 아래 이미지가 포함될 수 있습니다:\n",
    "  질문: {dict[\"question\"]}\n",
    "\n",
    "  텍스트와 표:\n",
    "  {format_texts}\n",
    "  \"\"\"\n",
    "\n",
    "  prompt = [\n",
    "      HumanMessage(\n",
    "          content=[\n",
    "              # 벡터 DB의 텍스트형 데이터는 질문과 같이 전달\n",
    "              {\"type\": \"text\", \"text\": text},\n",
    "              # 이미지 하나만 전달\n",
    "              {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{dict['context']['images'][0]}\"}},\n",
    "          ]\n",
    "      )\n",
    "  ]\n",
    "\n",
    "  return prompt\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o\", max_tokens=1024)\n",
    "\n",
    "# RAG 파이프라인\n",
    "# 1. 질문을 retriever에 넘겨 rag 검색\n",
    "# 2. 그 결과를 split_image_text_types에 넘겨 검색 결과의 텍스트와 이미지를 분리\n",
    "# 3. 결과가 context에 저장, 사용자의 질문인 question은 그냥 다음 단계로 넘김\n",
    "# 4. 맨위 {}안에서 일어난 일들로 context, question dict이 만들어지고 prompt_func에 넘겨짐\n",
    "# 최종 결과 예상\n",
    "# {\n",
    "#   \"context\": {\n",
    "#     \"texts\": [\"문맥 텍스트1\", \"문맥 텍스트2\", ...],\n",
    "#     \"images\": [\"base64-이미지-문자열\", ...]\n",
    "#   },\n",
    "#   \"question\": \"사용자 질문\"\n",
    "# }\n",
    "chain = (\n",
    "    {\"context\": retriever | RunnableLambda(split_image_text_types), \"question\": RunnablePassthrough()}\n",
    "    # 프롬프트 생성\n",
    "    | RunnableLambda(prompt_func)\n",
    "    # 모델 전달\n",
    "    | model\n",
    "    # 결과 처리\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chain.invoke(\n",
    "    \"말라리아 군집 사례는 어떤가요?\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **텍스트 및 테이블 요약**"
   ],
   "metadata": {
    "id": "zOoBa-wvmHcd",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 프롬프트 설정\n",
    "prompt_text = \"\"\"당신은 표와 텍스트를 요약하여 검색할 수 있도록 돕는 역할을 맡은 어시스턴트입니다.\n",
    "이 요약은 임베딩되어 원본 텍스트나 표 요소를 검색하는 데 사용될 것입니다.\n",
    "표 또는 텍스트에 대한 간결한 요약을 제공하여 검색에 최적화된 형태로 만들어 주세요. 표 또는 텍스트: {element} \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# 텍스트 요약 체인\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "# 제공된 텍스트에 대해 요약을 할 경우\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "\n",
    "# 요약을 원치 않을 경우\n",
    "# text_summaries = texts\n",
    "\n",
    "# 제공된 테이블에 적용\n",
    "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
   ],
   "metadata": {
    "id": "kloFpOLEmJyu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "table_summaries[0]\n",
    "text_summaries[0]"
   ],
   "metadata": {
    "id": "JIUNtvZsppaE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **base64 인코딩 전달**"
   ],
   "metadata": {
    "id": "cizMDRHYSE4e",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import base64\n",
    "\n",
    "# -> str = 타입 힌트, 반환값\n",
    "def encode_image(image_path) -> str:\n",
    "  # 이미지 base64 인코딩\n",
    "  # rb = 바이너리 읽기 모드\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    # .decode('utf-8') = 바이트를 문자열로 변환\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# 이미지의 base64 인코딩을 저장하는 리스트\n",
    "img_base64_list = []\n",
    "\n",
    "# 이미지를 읽어 bese64 인코딩 후 저장\n",
    "# os.listdir(fpath) = 해당 경로에 있는 파일 목록 불러오기\n",
    "for img_file in sorted(os.listdir(fpath)):\n",
    "  if img_file.endswith('.jpg')\n",
    "    img_path = os.path.join(fpath, img_file)\n",
    "    base64_image = encode_image(img_path)\n",
    "    img_base64_list.append(base64_image)"
   ],
   "metadata": {
    "id": "fGA2QAuASGAt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **이미지 요약문 생성**"
   ],
   "metadata": {
    "id": "oWbz9wUwV7GQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def image_summarize(img_base64: str) -> str:\n",
    "  # 이미지 요약\n",
    "  chat = ChatOpenAI(model=\"gpt-4o\", max_tokens=1024)\n",
    "  prompt = \"\"\"\n",
    "  당신은 이미지를 요약하여 검색을 위해 사용할 수 있도록 돕는 어시스턴트입니다.\n",
    "  이 요약은 임베딩되어 원본 이미지를 검색하는 데 사용됩니다.\n",
    "  이미지 검색에 최적화된 간결한 요약을 작성하세요.\n",
    "  \"\"\"\n",
    "  msg = chat.invoke(\n",
    "      [\n",
    "          HumanMessage(\n",
    "              content=[\n",
    "                  {\"type\": \"text\", \"text\": prompt},\n",
    "                  {\n",
    "                      \"type\": \"image_url\",\n",
    "                      \"image_url\": {\n",
    "                          \"url\": f\"data:image/jpeg;base64,{img_base64}\"\n",
    "                      }\n",
    "                  }\n",
    "              ]\n",
    "          )\n",
    "      ]\n",
    "  )\n",
    "\n",
    "  return msg.content\n",
    "\n",
    "# 이미지 요약을 저장하는 리스트\n",
    "image_summaries = []\n",
    "\n",
    "for img_base64 in img_base64_list:\n",
    "  image_summary = image_summarize(img_base64)"
   ],
   "metadata": {
    "id": "UdYboZlkV9Lw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "image_summaries[0]"
   ],
   "metadata": {
    "id": "PxEyDWsPaibf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **벡터 저장소**"
   ],
   "metadata": {
    "id": "mKlvS6CIbfb4",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.retrievers import MultiVectorRetriever\n",
    "from langchain_core.stores import InMemoryStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# 분할한 텍스트들을 색인할 벡터 저장소\n",
    "vectorstore = Chroma(collection_name=\"multi_modal_rag\", embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# 원본 문서 저장을 위한 저장소 선언\n",
    "# docstore는 실제 텍스트나 원본 자료(요약 전 문장, 이미지 정보 등) 를 저장하는 역할\n",
    "docstore = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# 검색기\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=docstore,\n",
    "    id_key=id_key,\n",
    ")"
   ],
   "metadata": {
    "id": "Zf-6vXs4cKmz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **벡터 저장소 저장**"
   ],
   "metadata": {
    "id": "bwHOA3eRhkYL",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import uuid\n",
    "\n",
    "# 원본 텍스트 데이터 저장\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "# 원본 테이블 데이터 저장\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "\n",
    "# 원본 이미지(base64) 데이터 저장\n",
    "img_ids = [str(uuid.uuid4()) for _ in img_base64_list]\n",
    "retriever.docstore.mset(list(zip(img_ids, img_base64_list)))"
   ],
   "metadata": {
    "id": "-uhcdGeIhlIu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.schema.document import Document\n",
    "\n",
    "# 텍스트 요약 벡터 저장\n",
    "summary_texts = [\n",
    "    # LangChain의 Document는 텍스트 + 메타데이터를 함께 담는 기본 단위\n",
    "    # 각 루프마다 Document 객체가 만들어지고 i와 s는 text_summaries를 순회하면서 추출\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    # i는 순회 인덱스, s는 컨텐츠\n",
    "    for i, s in enumerate(text_summaries)\n",
    "]\n",
    "\n",
    "# 즉 인덱스 리스트와 원본, 요약 벡터 데이터의 순서가 같아서 id값과 벡터 데이터를 맞추며 저장하느 ㄴ과정\n",
    "\n",
    "retriever.vecterstore.add_documents(summary_texts)\n",
    "\n",
    "# 테이블 요약 벡터 저장\n",
    "summary_tables = [\n",
    "    Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "    for i, s in enumerate(table_summaries)\n",
    "]\n",
    "\n",
    "retriever.vectorstore.add_documents(summary_table)\n",
    "\n",
    "# 이미지 요약 벡터 저장\n",
    "summary_img = [\n",
    "    Document(page_content=s, metadata={id_key: img_ids[i]})\n",
    "    for i, s in enumerate(image_summaries)\n",
    "]\n",
    "\n",
    "retriever.vectorstore.add_documents(summary_img)"
   ],
   "metadata": {
    "id": "izK74VwNus_d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **검색 결과 확인**"
   ],
   "metadata": {
    "id": "4oF4OLRnzob3",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "docs = retriever.invoke(\n",
    "    \"말라리아 군집 사례는 어떤가요?\"\n",
    ")\n",
    "\n",
    "len(docs)\n",
    "\n",
    "# 4\n",
    "# 4개의 검색 결과"
   ],
   "metadata": {
    "id": "6a5tj6VOzqvr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from base64 import b64decode\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "  # 이미지와 텍스트 데이터를 분리\n",
    "  b64 = []\n",
    "  text = []\n",
    "\n",
    "  for doc in docs:\n",
    "    try:\n",
    "      b64decode(doc)\n",
    "      b64.append(doc)\n",
    "    except Exception as e:\n",
    "      text.append(doc)\n",
    "\n",
    "  return {\n",
    "      \"images\": b64,\n",
    "      \"texts\": text\n",
    "  }\n",
    "\n",
    "docs_by_type = split_image_text_types(docs)\n",
    "\n",
    "len(docs_by_type[\"images\"])\n",
    "# 3\n",
    "\n",
    "len(docs_by_type[\"texts\"])\n",
    "# 1"
   ],
   "metadata": {
    "id": "cr3WAZohz7zL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "frp, IPython.display import display, HTML\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "  # base64 이미지로 html 태그를 작성\n",
    "  image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "\n",
    "  # html 태그를 기반으로 이미지를 표기\n",
    "  display(HTML(image_html))\n",
    "\n",
    "plt_img_base64(docs_by_type[\"images\"][0])"
   ],
   "metadata": {
    "id": "ZU4nrPa10p0Z",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "docs_by_type[\"texts\"][0]"
   ],
   "metadata": {
    "id": "kuQjhGL11fzB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **답변 생성**"
   ],
   "metadata": {
    "id": "hX8og7QB7Wsy",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from operator import itemgetter\n",
    "# RunnablePassthrough(): 입력을 그대로 다음 단계로 넘겨주는 역할(일종의 식별자/포맷 보존)\n",
    "# RunnableLambda(fn): 사용자가 제공한 함수 fn을 래핑해서 runnable로 만든다. 실행 시 해당 함수를 호출합니다.\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "def prompt_func(dict):\n",
    "  # 문맥 텍스트 리스트를 줄바꿈으로 연결해 하나의 긴 텍스트로 만든다\n",
    "  # 검색 결과들 합침\n",
    "  format_texts = \"\\n\".join(dict[\"context\"][\"texts\"])\n",
    "  text = f\"\"\"\n",
    "  다음 문맥에만 기반하여 질문에 답하세요. 문맥에는 텍스트, 표, 그리고 아래 이미지가 포함될 수 있습니다:\n",
    "  질문: {dict[\"question\"]}\n",
    "\n",
    "  텍스트와 표:\n",
    "  {format_texts}\n",
    "  \"\"\"\n",
    "\n",
    "  prompt = [\n",
    "      HumanMessage(\n",
    "          content=[\n",
    "              # 벡터 DB의 텍스트형 데이터는 질문과 같이 전달\n",
    "              {\"type\": \"text\", \"text\": text},\n",
    "              # 이미지 하나만 전달\n",
    "              {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{dict['context']['images'][0]}\"}},\n",
    "          ]\n",
    "      )\n",
    "  ]\n",
    "\n",
    "  return prompt\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o\", max_tokens=1024)\n",
    "\n",
    "# RAG 파이프라인\n",
    "# 1. 질문을 retriever에 넘겨 rag 검색\n",
    "# 2. 그 결과를 split_image_text_types에 넘겨 검색 결과의 텍스트와 이미지를 분리\n",
    "# 3. 결과가 context에 저장, 사용자의 질문인 question은 그냥 다음 단계로 넘김\n",
    "# 4. 맨위 {}안에서 일어난 일들로 context, question dict이 만들어지고 prompt_func에 넘겨짐\n",
    "# 최종 결과 예상\n",
    "# {\n",
    "#   \"context\": {\n",
    "#     \"texts\": [\"문맥 텍스트1\", \"문맥 텍스트2\", ...],\n",
    "#     \"images\": [\"base64-이미지-문자열\", ...]\n",
    "#   },\n",
    "#   \"question\": \"사용자 질문\"\n",
    "# }\n",
    "chain = (\n",
    "    {\"context\": retriever | RunnableLambda(split_image_text_types), \"question\": RunnablePassthrough()}\n",
    "    # 프롬프트 생성\n",
    "    | RunnableLambda(prompt_func)\n",
    "    # 모델 전달\n",
    "    | model\n",
    "    # 결과 처리\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "metadata": {
    "id": "iOksdh907YLy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "chain.invoke(\n",
    "    \"말라리아 군집 사례는 어떤가요?\"\n",
    ")"
   ],
   "metadata": {
    "id": "DxoT6OarBJO0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}